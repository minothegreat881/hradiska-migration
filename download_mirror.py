"""
JednoduchÃ½ mirror downloader pre hradiska.sk
Stiahne celÃº strÃ¡nku vrÃ¡tane vÅ¡etkÃ½ch sÃºborov pre offline pouÅ¾itie
"""

import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pathlib import Path
from typing import Set
import re

class SimpleMirror:
    def __init__(self, base_url: str = "http://www.hradiska.sk/", output_dir: str = "backup/hradiska_mirror"):
        self.base_url = base_url
        self.domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.visited_urls: Set[str] = set()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        # Å tatistiky
        self.stats = {
            'html': 0,
            'images': 0,
            'css': 0,
            'js': 0,
            'other': 0,
            'failed': 0
        }

    def clean_filename(self, url: str) -> Path:
        """VytvorÃ­ bezpeÄnÃº cestu k sÃºboru z URL"""
        import hashlib

        parsed = urlparse(url)
        path = parsed.path

        # Ak je to root, pouÅ¾ij index.html
        if not path or path == '/':
            return self.output_dir / 'index.html'

        # OdstrÃ¡nenie ÃºvodnÃ©ho /
        path = path.lstrip('/')

        # Ak cesta konÄÃ­ na /, pridaj index.html
        if path.endswith('/'):
            path += 'index.html'
        elif '.' not in Path(path).name:
            # Ak nemÃ¡ prÃ­ponu, pridaj .html
            path += '.html'

        # Nahradenie nebezpeÄnÃ½ch znakov
        path = path.replace('?', '_').replace('&', '_').replace('=', '_')

        # FIX pre dlhÃ© URL (Windows limit 260 znakov)
        # Ak je cesta prÃ­liÅ¡ dlhÃ¡, pouÅ¾ij hash
        full_path = self.output_dir / path
        if len(str(full_path)) > 200:  # BezpeÄnÃ¡ rezerva
            # Zachovaj prÃ­ponu
            ext = Path(path).suffix or '.html'
            # Vytvor hash z celej URL
            url_hash = hashlib.md5(url.encode()).hexdigest()[:12]

            # Pre obrÃ¡zky daj do images/, pre HTML do pages/
            if ext.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:
                path = f"images/{url_hash}{ext}"
            elif ext.lower() in ['.css']:
                path = f"css/{url_hash}{ext}"
            elif ext.lower() in ['.js']:
                path = f"js/{url_hash}{ext}"
            else:
                path = f"pages/{url_hash}{ext}"

        return self.output_dir / path

    def download_file(self, url: str) -> bool:
        """Stiahne sÃºbor z URL"""
        try:
            response = self.session.get(url, timeout=30, verify=False)
            response.raise_for_status()

            # UrÄenie cieÄ¾ovej cesty
            filepath = self.clean_filename(url)
            filepath.parent.mkdir(parents=True, exist_ok=True)

            # UloÅ¾enie sÃºboru
            with open(filepath, 'wb') as f:
                f.write(response.content)

            # AktualizÃ¡cia Å¡tatistÃ­k
            content_type = response.headers.get('content-type', '').lower()
            if 'html' in content_type:
                self.stats['html'] += 1
            elif 'image' in content_type:
                self.stats['images'] += 1
            elif 'css' in content_type:
                self.stats['css'] += 1
            elif 'javascript' in content_type:
                self.stats['js'] += 1
            else:
                self.stats['other'] += 1

            return True, response.content, content_type

        except Exception as e:
            self.stats['failed'] += 1
            print(f"  âŒ Chyba: {str(e)[:50]}")
            return False, None, None

    def get_links_from_html(self, html_content: bytes, base_url: str) -> Set[str]:
        """Extrahuje vÅ¡etky odkazy z HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            links = set()

            # StrÃ¡nky
            for tag in soup.find_all('a', href=True):
                href = tag['href']
                full_url = urljoin(base_url, href)
                # Len strÃ¡nky z rovnakej domÃ©ny
                if urlparse(full_url).netloc == self.domain:
                    # OdstrÃ¡nenie fragmentov (#)
                    full_url = full_url.split('#')[0]
                    if full_url:
                        links.add(full_url)

            # ObrÃ¡zky
            for tag in soup.find_all('img', src=True):
                src = tag['src']
                full_url = urljoin(base_url, src)
                links.add(full_url)

            # CSS
            for tag in soup.find_all('link', href=True):
                if tag.get('rel') == ['stylesheet'] or '.css' in tag['href']:
                    href = tag['href']
                    full_url = urljoin(base_url, href)
                    links.add(full_url)

            # JavaScript
            for tag in soup.find_all('script', src=True):
                src = tag['src']
                full_url = urljoin(base_url, src)
                links.add(full_url)

            # Background images z CSS
            for tag in soup.find_all(style=True):
                style = tag['style']
                urls = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', style)
                for url in urls:
                    full_url = urljoin(base_url, url)
                    links.add(full_url)

            return links

        except Exception as e:
            print(f"  âš ï¸  Chyba pri parsovanÃ­ HTML: {e}")
            return set()

    def print_progress(self):
        """ZobrazÃ­ progress"""
        total = sum([self.stats['html'], self.stats['images'],
                    self.stats['css'], self.stats['js'], self.stats['other']])
        print(f"\r  ğŸ“Š StiahnutÃ©: {total} sÃºborov "
              f"(HTML: {self.stats['html']}, "
              f"ObrÃ¡zky: {self.stats['images']}, "
              f"CSS: {self.stats['css']}, "
              f"JS: {self.stats['js']}, "
              f"ZlyhanÃ©: {self.stats['failed']})", end='', flush=True)

    def mirror_website(self, max_pages: int = 1000):
        """Stiahne celÃº webstrÃ¡nku"""
        print(f"ğŸŒ ZaÄÃ­nam sÅ¥ahovanie: {self.base_url}")
        print(f"ğŸ“ CieÄ¾ovÃ½ prieÄinok: {self.output_dir.absolute()}")
        print()

        to_download = {self.base_url}
        downloaded = 0

        while to_download and downloaded < max_pages:
            url = to_download.pop()

            if url in self.visited_urls:
                continue

            self.visited_urls.add(url)
            downloaded += 1

            # Zobrazenie aktuÃ¡lneho URL
            short_url = url.replace(self.base_url, '')[:60]
            print(f"\nğŸ“¥ [{downloaded}/{max_pages}] {short_url}")

            # Stiahnutie
            success, content, content_type = self.download_file(url)

            if success and content and 'html' in (content_type or ''):
                # Extrakcia ÄalÅ¡Ã­ch odkazov
                new_links = self.get_links_from_html(content, url)
                to_download.update(new_links - self.visited_urls)

            # Progress
            self.print_progress()

            # Pauza medzi poÅ¾iadavkami
            time.sleep(0.3)

        print("\n")
        print("=" * 50)
        print("âœ… SÅ¤AHOVANIE DOKONÄŒENÃ‰!")
        print("=" * 50)
        print(f"ğŸ“Š Å tatistiky:")
        print(f"  â€¢ HTML strÃ¡nok: {self.stats['html']}")
        print(f"  â€¢ ObrÃ¡zkov: {self.stats['images']}")
        print(f"  â€¢ CSS sÃºborov: {self.stats['css']}")
        print(f"  â€¢ JS sÃºborov: {self.stats['js']}")
        print(f"  â€¢ OstatnÃ½ch: {self.stats['other']}")
        print(f"  â€¢ Zlyhalo: {self.stats['failed']}")
        print(f"  â€¢ CELKOM: {sum([self.stats['html'], self.stats['images'], self.stats['css'], self.stats['js'], self.stats['other']])} sÃºborov")
        print()
        print(f"ğŸ“ SÃºbory uloÅ¾enÃ© v: {self.output_dir.absolute()}")

def main():
    """HlavnÃ¡ funkcia"""
    # Nastavenie UTF-8 encoding pre Windows
    import sys
    import io
    if sys.platform == 'win32':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

    # Vypnutie SSL varovanÃ­
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    print()
    print("=" * 50)
    print("  HRADISKA.SK - MIRROR DOWNLOADER")
    print("=" * 50)
    print()

    mirror = SimpleMirror()

    try:
        mirror.mirror_website(max_pages=1000)

        print()
        print("ğŸš€ ÄALÅ IE KROKY:")
        print("1. Spustite: start_local_server.bat")
        print("2. Otvorte prehliadaÄ: http://localhost:8000")
        print()
        print("ğŸ’¡ VÅ¡etky sÃºbory sÃº lokÃ¡lne a fungujÃº offline!")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  SÅ¥ahovanie preruÅ¡enÃ© uÅ¾Ã­vateÄ¾om")
        print(f"StiahnutÃ½ch {len(mirror.visited_urls)} sÃºborov pred preruÅ¡enÃ­m")
    except Exception as e:
        print(f"\n\nâŒ Chyba: {e}")
        return 1

    return 0

if __name__ == "__main__":
    sys.exit(main())